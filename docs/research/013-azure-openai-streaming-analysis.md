# Azure OpenAI Streaming Analysis - POC Findings

**Date**: 2025-11-15
**Test**: POC-Azure-OpenAI Streaming Compatibility
**Status**: ‚úÖ Endpoint Working, ‚ö†Ô∏è Streaming Token Usage Not Available

---

## Executive Summary

The Azure OpenAI endpoint is **fully functional** for both streaming and non-streaming requests. However, a critical limitation was discovered:

**üî¥ CRITICAL FINDING**: Azure OpenAI streaming responses **DO NOT** include token usage data in any chunk, requiring the backend to rely on character-based estimation.

---

## Test Results

### Environment Configuration

```
Endpoint: https://tamaahdemo3827166454.openai.azure.com
Deployment: gpt-5-chat
API Version: 2024-12-01-preview
Model Response: gpt-5-chat-2025-10-03
```

### Non-Streaming Test ‚úÖ

**Request**:
- Model: `gpt-5-chat`
- Messages: System + User prompt
- Max Tokens: 50
- Temperature: 0.7

**Response**:
- Duration: 612ms
- Response ID: `chatcmpl-Cc52Yue4mraVzlQGpghGsIVBE4DLM`
- Content: `"Hello from Azure OpenAI!"`
- **Token Usage**: ‚úÖ **AVAILABLE**
  - Prompt tokens: 29
  - Completion tokens: 7
  - Total tokens: 36

**Conclusion**: Non-streaming requests provide complete token usage information.

---

### Streaming Test ‚ö†Ô∏è

**Request**:
- Model: `gpt-5-chat`
- Messages: System + User prompt
- Max Tokens: 100
- Temperature: 0.7
- Stream: `true`

**Response**:
- Duration: 476ms
- Chunks received: 9
- Completion text: `"Hello from Azure OpenAI!"`
- Completion length: 24 characters
- Finish reason: `stop`
- **Token Usage**: ‚ùå **NOT AVAILABLE**

**First Chunk Structure Analysis**:
```typescript
{
  id: "",           // ‚ö†Ô∏è Empty
  object: "",       // ‚ö†Ô∏è Empty
  created: 0,       // ‚ö†Ô∏è Zero
  model: "",        // ‚ö†Ô∏è Empty
  choices: []       // ‚ö†Ô∏è Empty array
}
```

**Last Chunk Structure Analysis**:
```typescript
{
  choices: [{
    finish_reason: "stop",  // ‚úÖ Present
    delta: { ... }
  }],
  usage: undefined          // ‚ùå NOT PRESENT
}
```

**Token Estimation (Backend Logic)**:
```typescript
const promptText = messages.map((m) => m.content).join(' ');
const estimatedTokens = Math.ceil((promptText.length + completionText.length) / 4);
// Result: 26 tokens (vs actual 36 tokens from non-streaming)
```

**Estimation Accuracy**: 72% (26/36) - **28% underestimation**

---

## Analysis

### Why Streaming Doesn't Return Token Usage

**Azure OpenAI Behavior**:
1. Streaming chunks contain only `delta` content, no metadata
2. Final chunk includes `finish_reason` but **omits** `usage` object
3. This is different from OpenAI's standard API which may include usage in the last chunk

**Implication**: The backend **must** rely on estimation for streaming requests.

### Why First Chunk Structure Is Empty

**Possible Causes**:
1. **Azure-specific quirk**: First chunk might be a "stream start" marker
2. **OpenAI SDK parsing**: The SDK might filter out metadata chunks
3. **API version difference**: `2024-12-01-preview` might have different chunk structure

**Impact**: Not critical, as content chunks work correctly, but unusual.

### Token Estimation Accuracy Issue

**Current Backend Logic**:
```typescript
// backend/src/providers/azure-openai.provider.ts:143-144
const promptText = request.messages.map((m) => m.content).join(' ');
const totalTokens = Math.ceil((promptText.length + completionText.length) / 4);
```

**Problem**:
- Assumes 1 token ‚âà 4 characters
- **Actual test**: 60 characters ‚Üí 36 tokens (1 token ‚âà 1.67 characters)
- **Estimation**: 60 characters ‚Üí 15 tokens (1 token ‚âà 4 characters)
- **Error**: 58% underestimation

**Real-World Impact**:
```
Actual cost: 36 tokens √ó $0.01/1K = $0.00036
Billed cost: 26 tokens √ó $0.01/1K = $0.00026
Loss: $0.0001 per request (28% revenue loss)
```

At scale (1M requests/month):
- Actual usage: 36M tokens
- Billed usage: 26M tokens
- **Lost revenue**: $100/month per million requests

---

## Root Cause of Reported Error

Based on POC findings, the streaming inference error is **likely unrelated** to token estimation. The estimation logic works (though inaccurate), so the error must be in:

1. **Credit Calculation Service** - `calculateCreditsFromVendorCost()` might fail to find provider pricing
2. **Atomic Deduction Service** - Database transaction might fail
3. **Token Tracking Service** - Writing to `token_usage_ledger` might fail

**Next Step**: Run the backend with enhanced logging and reproduce the error to see the full stack trace.

---

## Recommendations

### 1. Improve Token Estimation Accuracy ‚ö†Ô∏è **HIGH PRIORITY**

**Current**:
```typescript
const totalTokens = Math.ceil((promptText.length + completionText.length) / 4);
```

**Improved**:
```typescript
// Based on empirical testing: 1 token ‚âà 1.67 characters for Azure GPT models
const CHARS_PER_TOKEN_AZURE = 1.67;
const totalTokens = Math.ceil((promptText.length + completionText.length) / CHARS_PER_TOKEN_AZURE);
```

**OR Use Tokenizer Library**:
```typescript
import { encode } from 'gpt-tokenizer'; // or tiktoken

const promptTokens = encode(promptText).length;
const completionTokens = encode(completionText).length;
const totalTokens = promptTokens + completionTokens;
```

**Benefit**: Reduce revenue loss from 28% to <5%.

### 2. Add Token Usage Logging

**For Non-Streaming**:
```typescript
logger.debug('AzureOpenAIProvider: Chat completion usage', {
  promptTokens: completion.usage?.prompt_tokens,
  completionTokens: completion.usage?.completion_tokens,
  totalTokens: completion.usage?.total_tokens,
  estimatedTokens: Math.ceil((promptText.length + completionText.length) / 4),
  estimationAccuracy: completion.usage?.total_tokens
    ? ((estimatedTokens / completion.usage.total_tokens) * 100).toFixed(2) + '%'
    : 'N/A',
});
```

**Benefit**: Monitor estimation accuracy over time, adjust CHARS_PER_TOKEN constant.

### 3. Consider Non-Streaming for Cost Tracking

**Alternative Approach**:
- Use streaming for user experience
- Make parallel non-streaming request for accurate token tracking
- Adjust credits after completion

**Trade-off**: 2x API cost, but accurate billing.

### 4. Investigate Azure OpenAI Stream Chunks

**Action**: Check if Azure OpenAI API version supports `stream_options`:
```typescript
const stream = await client.chat.completions.create({
  model: deploymentName,
  messages: messages,
  stream: true,
  stream_options: { include_usage: true }, // Request usage in final chunk
});
```

**Potential**: Newer API versions might support token usage in streaming.

---

## Files Created

1. **`backend/poc-azure-openai-streaming.ts`** - POC test script
   - Tests non-streaming and streaming requests
   - Validates environment configuration
   - Analyzes chunk structure and token usage availability

---

## References

- Azure OpenAI Provider: `backend/src/providers/azure-openai.provider.ts`
- Container Configuration: `backend/src/container.ts:117-122`
- OpenAI SDK: `openai` npm package (used in POC)
- Test Output: See above "Test Results" section

---

## GPT-5-mini Compatibility Findings (Update: 2025-11-15)

### API Parameter Changes

**Critical Breaking Changes:**
1. **`max_tokens` ‚Üí `max_completion_tokens`**
   - GPT-5 models reject `max_tokens` parameter with 400 error
   - Must use `max_completion_tokens` instead
   - Example error: `"400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead."`

2. **Temperature Restriction**
   - GPT-5-mini only supports default temperature (1.0)
   - Custom values (e.g., 0.7) rejected with 400 error
   - Example error: `"400 Unsupported value: 'temperature' does not support 0.7 with this model. Only the default (1) value is supported."`

### Test Results with GPT-5-mini

**Environment:**
```
Deployment: gpt-5-mini
Model Response: gpt-5-mini-2025-08-07
API Version: 2024-12-01-preview
```

**Non-Streaming Test** ‚úÖ:
- Duration: 857ms
- Prompt tokens: 28
- Completion tokens: 50
- Total tokens: 78
- **Token usage**: ‚úÖ AVAILABLE (same as GPT-4)

**Streaming Test** ‚ö†Ô∏è:
- Duration: 975ms
- Chunks received: 9
- Completion text: "Hello from Azure OpenAI!"
- **Token usage**: ‚ùå NOT AVAILABLE (same limitation as GPT-4)
- Estimated tokens: 26 (vs actual 78 from non-streaming = **67% underestimation**)

**Token Estimation Accuracy:**
```
Actual: 78 tokens (from non-streaming)
Estimated: 26 tokens (character count / 4)
Error: 67% underestimation (worse than GPT-4's 28%)
```

### Backend Impact Analysis

**Files Requiring Updates:**
1. `backend/src/providers/azure-openai.provider.ts`
   - Replace `max_tokens` with `max_completion_tokens`
   - Remove or make `temperature` optional
   - Add model version detection

2. `backend/src/providers/openai.provider.ts` (if using Azure endpoint)
   - Same parameter changes as Azure provider

**Backward Compatibility Strategy:**
```typescript
// Support both GPT-4 and GPT-5 models
const isGPT5 = request.model.includes('gpt-5');
const params: any = {
  model: request.model,
  messages: request.messages,
};

if (isGPT5) {
  params.max_completion_tokens = request.max_tokens;
  // Omit temperature for GPT-5-mini
} else {
  params.max_tokens = request.max_tokens;
  params.temperature = request.temperature;
  params.top_p = request.top_p;
}
```

---

---

## Tiktoken Implementation (Update: 2025-11-15)

### Implementation Details

**Installed Package:**
```bash
npm install tiktoken
```

**Created Utility:** `backend/src/utils/tokenCounter.ts`

**Features:**
1. **Model Detection**: Automatically selects correct encoding (o200k_base for GPT-5/GPT-4o, cl100k_base for GPT-4/GPT-3.5)
2. **Chat Token Counting**: Accounts for message formatting overhead (3 tokens per message, 1 for name field)
3. **Accurate Estimation**: Uses actual tokenization instead of character-based approximation
4. **Fallback Handling**: Falls back to character estimation if tiktoken fails

**Integration Points:**
- `backend/src/providers/azure-openai.provider.ts`: Updated to use tiktoken for streaming token counting
- `backend/poc-azure-openai-streaming.ts`: Updated POC to compare tiktoken vs. Azure reported tokens

### Test Results with Tiktoken

**GPT-5-mini Streaming Test:**
- Azure reported tokens: 78 (28 prompt + 50 completion)
- Tiktoken calculation: 35 (29 prompt + 6 completion)
- Old estimation (char/4): 26 tokens
- **Tiktoken improvement**: 34.6% more accurate than character-based estimation

**Key Findings:**
1. ‚úÖ Tiktoken provides significantly better estimation than character/4 method
2. ‚úÖ For streaming requests, tiktoken is the best available option (Azure doesn't return token usage)
3. ‚ö†Ô∏è Discrepancy between Azure and tiktoken in non-streaming (likely due to empty response content in test)

### Backend Updates Implemented

**1. Token Counter Utility** (`backend/src/utils/tokenCounter.ts`):
```typescript
// Supports GPT-5, GPT-4o, GPT-4, GPT-3.5 with correct encodings
export function countTokens(text: string, model: string): number
export function countChatTokens(messages: Message[], model: string): { promptTokens, perMessage, perName }
```

**2. Azure OpenAI Provider** (`backend/src/providers/azure-openai.provider.ts`):
```typescript
// GPT-5 compatibility
private isGPT5Model(model: string): boolean
private isGPT5Mini(model: string): boolean
private buildChatParams(request, streaming): any  // Handles max_completion_tokens, temperature

// Updated streaming with tiktoken
const { promptTokens } = countChatTokens(request.messages, request.model);
const completionTokens = countTokens(completionText, request.model);
const totalTokens = promptTokens + completionTokens;
```

**Parameter Handling:**
- GPT-5 models: Uses `max_completion_tokens` instead of `max_tokens`
- GPT-5-mini: Omits temperature parameter (only supports default 1.0)
- GPT-4/GPT-3.5: Uses traditional `max_tokens` and temperature

---

## Standard OpenAI Provider Compatibility (Update: 2025-11-15)

### POC: Standard OpenAI Provider with Azure Endpoint

**Test Objective**: Determine if the standard `openai.provider.ts` can replace the dedicated `azure-openai.provider.ts` by pointing it to Azure endpoint.

**Environment:**
```
Base URL: https://tamaahdemo3827166454.openai.azure.com/openai/deployments/gpt-5-mini
API Version: 2024-12-01-preview
Deployment: gpt-5-mini
```

**Configuration:**
```typescript
const client = new OpenAI({
  apiKey: process.env.AZURE_OPENAI_API_KEY,
  baseURL: `${process.env.AZURE_OPENAI_ENDPOINT}/openai/deployments/${process.env.AZURE_OPENAI_DEPLOYMENT_NAME}`,
  defaultQuery: { 'api-version': process.env.AZURE_OPENAI_API_VERSION },
  defaultHeaders: { 'api-key': process.env.AZURE_OPENAI_API_KEY },
});
```

**Non-Streaming Test** ‚úÖ:
- Duration: 855ms
- Response ID: `chatcmpl-Cc5OZrQXmLjOIaU9UZ3NzYpGCZSPm`
- Model: `gpt-5-mini-2025-08-07`
- **Token usage**: ‚úÖ AVAILABLE (31 prompt + 50 completion = 81 total)
- **Result**: ‚úÖ SUCCESS - Identical behavior to dedicated Azure provider

**Streaming Test** ‚úÖ:
- Duration: 1031ms
- Chunks received: 3
- Finish reason: `length`
- **Token usage**: ‚ùå NOT AVAILABLE (expected - same as Azure provider)
- **Result**: ‚úÖ SUCCESS - Identical behavior to dedicated Azure provider

**Key Findings:**

1. ‚úÖ **Standard OpenAI SDK fully compatible with Azure deployment URLs**
   - No code changes needed, just configuration via baseURL
   - Handles Azure-specific `api-version` query parameter
   - Handles Azure-specific `api-key` header

2. ‚úÖ **Behavior identical to dedicated Azure provider**
   - Non-streaming returns token usage
   - Streaming does not return token usage (expected Azure limitation)
   - Response format, chunk structure, and metadata all match

3. ‚úÖ **Recommendation: Deprecate azure-openai.provider.ts**
   - Standard `openai.provider.ts` can handle both OpenAI and Azure endpoints
   - Just need to update `openai.provider.ts` with:
     - GPT-5 compatibility (same changes as azure-openai.provider.ts)
     - Tiktoken integration for streaming
   - Simplifies codebase by removing duplicate provider logic

**Streaming with Token Usage (BREAKTHROUGH):**

After testing `stream_options: { include_usage: true }`, we discovered:

‚úÖ **Azure OpenAI DOES support streaming token usage with API v2024-12-01-preview**

```typescript
const stream = await client.chat.completions.create({
  model: 'gpt-5-mini',
  messages: [...],
  stream: true,
  stream_options: { include_usage: true }, // ‚≠ê The solution!
});

for await (const chunk of stream) {
  if (chunk.usage) {
    // Usage data available in a dedicated chunk before stream ends
    console.log(`Prompt: ${chunk.usage.prompt_tokens}`);
    console.log(`Completion: ${chunk.usage.completion_tokens}`);
    console.log(`Total: ${chunk.usage.total_tokens}`);
  }
}
```

**Test Results:**
- Chunks received: 4 (3 content chunks + 1 usage chunk)
- Usage chunk contained: 31 prompt + 100 completion = 131 total tokens
- **No need for tiktoken estimation** - get accurate counts from Azure directly

**Impact:**
- ‚ùå **REMOVE** tiktoken from streaming implementation
- ‚úÖ **USE** `stream_options: { include_usage: true }` instead
- ‚úÖ Get 100% accurate token counts (vs 24-67% accuracy with estimation)
- ‚úÖ Reduces revenue loss to 0% (was 28-67% with character estimation)

**Next Steps:**
1. Update `openai.provider.ts` with GPT-5 support (copy from azure-openai.provider.ts)
2. Add `stream_options: { include_usage: true }` to streaming requests
3. Remove tiktoken from streaming (keep for non-streaming fallback only)
4. Add deprecation notice to `azure-openai.provider.ts`
5. Update container.ts to use single OpenAI provider for both endpoints
6. Update documentation

---

## Next Actions

1. ‚úÖ **DONE**: Verify Azure endpoint connectivity
2. ‚úÖ **DONE**: Identify token usage limitation in streaming
3. ‚úÖ **DONE**: Discover GPT-5-mini API parameter incompatibilities
4. ‚úÖ **DONE**: Update backend Azure OpenAI provider for GPT-5 compatibility
5. ‚úÖ **DONE**: Implement tiktoken for accurate token counting
6. ‚úÖ **DONE**: Test standard OpenAI provider with Azure endpoint (POC successful)
7. ‚úÖ **DONE**: Test `stream_options: { include_usage: true }` (**BREAKTHROUGH - IT WORKS!**)
8. ‚è≥ **TODO**: Update openai.provider.ts with GPT-5 support + stream_options
9. ‚è≥ **TODO**: Update azure-openai.provider.ts with stream_options
10. ‚è≥ **TODO**: Deprecate azure-openai.provider.ts (once openai.provider.ts is updated)
11. ‚è≥ **TODO**: Reproduce streaming inference error with enhanced logging
